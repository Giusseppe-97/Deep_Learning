You will write two helper functions to initialize the parameters for your model. 
The first function will be used to initialize parameters for a two layer model. 
The second one generalizes this initialization process to  𝐿  layers.


Initialize_parameters
1.1 Create and initialize the parameters of the 2-layer neural network.

Instructions:

The model's structure is: LINEAR -> RELU -> LINEAR -> SIGMOID.
Use this random initialization for the weight matrices: np.random.randn(shape)*0.01 with the correct shape
Use zero initialization for the biases: np.zeros(shape)

1.2 Implement initialization for an L-layer Neural Network.

Instructions:

The model's structure is *[LINEAR -> RELU]  ×  (L-1) -> LINEAR -> SIGMOID*. I.e., it has  𝐿−1  layers using a ReLU activation function followed by an output layer with a sigmoid activation function.
Use random initialization for the weight matrices. Use np.random.randn(shape) * 0.01.
Use zeros initialization for the biases. Use np.zeros(shape).
You'll store  𝑛[𝑙] , the number of units in different layers, in a variable layer_dims. For example, the layer_dims for last week's Planar Data classification model would have been [2,4,1]: There were two inputs, one hidden layer with 4 hidden units, and an output layer with 1 output unit. This means W1's shape was (4,2), b1 was (4,1), W2 was (1,4) and b2 was (1,1). Now you will generalize this to  𝐿  layers!
Here is the implementation for  𝐿=1  (one layer neural network). It should inspire you to implement the general case (L-layer neural network).
  if L == 1:
      parameters["W" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01
      parameters["b" + str(L)] = np.zeros((layer_dims[1], 1))