You will write two helper functions to initialize the parameters for your model. 
The first function will be used to initialize parameters for a two layer model. 
The second one generalizes this initialization process to  ğ¿  layers.


Initialize_parameters
1.1 Create and initialize the parameters of the 2-layer neural network.

Instructions:

The model's structure is: LINEAR -> RELU -> LINEAR -> SIGMOID.
Use this random initialization for the weight matrices: np.random.randn(shape)*0.01 with the correct shape
Use zero initialization for the biases: np.zeros(shape)

1.2 Implement initialization for an L-layer Neural Network.

Instructions:

The model's structure is *[LINEAR -> RELU]  Ã—  (L-1) -> LINEAR -> SIGMOID*. I.e., it has  ğ¿âˆ’1  layers using a ReLU activation function followed by an output layer with a sigmoid activation function.
Use random initialization for the weight matrices. Use np.random.randn(shape) * 0.01.
Use zeros initialization for the biases. Use np.zeros(shape).
You'll store  ğ‘›[ğ‘™] , the number of units in different layers, in a variable layer_dims. For example, the layer_dims for last week's Planar Data classification model would have been [2,4,1]: There were two inputs, one hidden layer with 4 hidden units, and an output layer with 1 output unit. This means W1's shape was (4,2), b1 was (4,1), W2 was (1,4) and b2 was (1,1). Now you will generalize this to  ğ¿  layers!
Here is the implementation for  ğ¿=1  (one layer neural network). It should inspire you to implement the general case (L-layer neural network).
  if L == 1:
      parameters["W" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01
      parameters["b" + str(L)] = np.zeros((layer_dims[1], 1))