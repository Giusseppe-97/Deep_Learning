 L-Model Backward¶
Now you will implement the backward function for the whole network!

Recall that when you implemented the L_model_forward function, at each iteration, you stored a cache which contains (X,W,b, and z).
In the back propagation module, you'll use those variables to compute the gradients. 
Therefore, in the L_model_backward function, you'll iterate through all the hidden layers backward, starting from layer  𝐿 . 
On each step, you will use the cached values for layer  𝑙  to backpropagate through layer  𝑙 . Figure 5 below shows the backward pass.


Figure 5: Backward pass
Initializing backpropagation:

To backpropagate through this network, you know that the output is:  𝐴[𝐿]=𝜎(𝑍[𝐿]) . 
Your code thus needs to compute dAL  =∂∂𝐴[𝐿] . To do so, use this formula (derived using calculus which, again, you don't need in-depth knowledge of!):

dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL
You can then use this post-activation gradient dAL to keep going backward. As seen in Figure 5, 
you can now feed in dAL into the LINEAR->SIGMOID backward function you implemented (which will use the cached values stored by the L_model_forward function).

After that, you will have to use a for loop to iterate through all the other layers using the LINEAR->RELU backward function. 
You should store each dA, dW, and db in the grads dictionary. To do so, use this formula :

𝑔𝑟𝑎𝑑𝑠["𝑑𝑊"+𝑠𝑡𝑟(𝑙)]=𝑑𝑊[𝑙](15)
For example, for  𝑙=3  this would store  𝑑𝑊[𝑙]  in grads["dW3"].