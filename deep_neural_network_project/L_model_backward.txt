 L-Model BackwardÂ¶
Now you will implement the backward function for the whole network!

Recall that when you implemented the L_model_forward function, at each iteration, you stored a cache which contains (X,W,b, and z).
In the back propagation module, you'll use those variables to compute the gradients. 
Therefore, in the L_model_backward function, you'll iterate through all the hidden layers backward, starting from layer  ğ¿ . 
On each step, you will use the cached values for layer  ğ‘™  to backpropagate through layer  ğ‘™ . Figure 5 below shows the backward pass.


Figure 5: Backward pass
Initializing backpropagation:

To backpropagate through this network, you know that the output is:  ğ´[ğ¿]=ğœ(ğ‘[ğ¿]) . 
Your code thus needs to compute dAL  =âˆ‚îˆ¸âˆ‚ğ´[ğ¿] . To do so, use this formula (derived using calculus which, again, you don't need in-depth knowledge of!):

dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL
You can then use this post-activation gradient dAL to keep going backward. As seen in Figure 5, 
you can now feed in dAL into the LINEAR->SIGMOID backward function you implemented (which will use the cached values stored by the L_model_forward function).

After that, you will have to use a for loop to iterate through all the other layers using the LINEAR->RELU backward function. 
You should store each dA, dW, and db in the grads dictionary. To do so, use this formula :

ğ‘”ğ‘Ÿğ‘ğ‘‘ğ‘ ["ğ‘‘ğ‘Š"+ğ‘ ğ‘¡ğ‘Ÿ(ğ‘™)]=ğ‘‘ğ‘Š[ğ‘™](15)
For example, for  ğ‘™=3  this would store  ğ‘‘ğ‘Š[ğ‘™]  in grads["dW3"].