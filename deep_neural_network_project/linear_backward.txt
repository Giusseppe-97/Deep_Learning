 Linear Backward
For layer  ğ‘™ , the linear part is:  ğ‘[ğ‘™]=ğ‘Š[ğ‘™]ğ´[ğ‘™âˆ’1]+ğ‘[ğ‘™]  (followed by an activation).

Suppose you have already calculated the derivative  ğ‘‘ğ‘[ğ‘™]=âˆ‚îˆ¸âˆ‚ğ‘[ğ‘™] . 
You want to get  (ğ‘‘ğ‘Š[ğ‘™],ğ‘‘ğ‘[ğ‘™],ğ‘‘ğ´[ğ‘™âˆ’1]) .


Figure Linear Backward
The three outputs  (ğ‘‘ğ‘Š[ğ‘™],ğ‘‘ğ‘[ğ‘™],ğ‘‘ğ´[ğ‘™âˆ’1])  are computed using the input  ğ‘‘ğ‘[ğ‘™] .

Here are the formulas you need:
ğ‘‘ğ‘Š[ğ‘™]=âˆ‚îˆ¶âˆ‚ğ‘Š[ğ‘™]=1ğ‘šğ‘‘ğ‘[ğ‘™]ğ´[ğ‘™âˆ’1]ğ‘‡(8)
ğ‘‘ğ‘[ğ‘™]=âˆ‚îˆ¶âˆ‚ğ‘[ğ‘™]=1ğ‘šâˆ‘ğ‘–=1ğ‘šğ‘‘ğ‘[ğ‘™](ğ‘–)(9)
ğ‘‘ğ´[ğ‘™âˆ’1]=âˆ‚îˆ¸âˆ‚ğ´[ğ‘™âˆ’1]=ğ‘Š[ğ‘™]ğ‘‡ğ‘‘ğ‘[ğ‘™](10)
ğ´[ğ‘™âˆ’1]ğ‘‡  is the transpose of  ğ´[ğ‘™âˆ’1] .

Linear-Activation BackwardÂ¶
Next, you will create a function that merges the two helper functions: 
linear_backward and the backward step for the activation linear_activation_backward.

To help you implement linear_activation_backward, two backward functions have been provided:

sigmoid_backward: Implements the backward propagation for SIGMOID unit. You can call it as follows:
dZ = sigmoid_backward(dA, activation_cache)
relu_backward: Implements the backward propagation for RELU unit. You can call it as follows:
dZ = relu_backward(dA, activation_cache)
If ğ‘”(.) is the activation function, sigmoid_backward and relu_backward compute
ğ‘‘ğ‘[ğ‘™]=ğ‘‘ğ´[ğ‘™]âˆ—ğ‘”â€²(ğ‘[ğ‘™])